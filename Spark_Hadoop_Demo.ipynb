{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark + Hadoop + H2O Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the complexities of code involved, most of the examples below can't actually be run from the notebook. Where noted, copy the code shown into an appropiate script and execute from the console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop\n",
    "\n",
    "### Hadoop Installation\n",
    "\n",
    "Follow the tutorial at https://www.edureka.co/blog/install-hadoop-single-node-hadoop-cluster . Some notes to consider:\n",
    "* Download and install JDK 8u101 (from https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html , the version from the tutorial may not work for your processor)\n",
    "* Download and install Hadoop\n",
    "* Configure environment for use\n",
    "   * This configuration consists of editing xml files so that Hadoop creates a single node environment. They're pretty easy to follow\n",
    "   * Before starting the daemons, run the command `sudo apt-get install openssh-client openssh-server`. This is required to open port 22\n",
    "   * At the end of this tutorial, you will have a Hadoop instance running. Browse to http://localhost:50070/dfshealth.html to see it in action.\n",
    "   * SH script to configure environment and launch Hadoop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#Setup Java\n",
    "export JAVA_HOME=$HOME/jdk1.8.0_101\n",
    "export PATH=$JAVA_HOME/bin:$PATH\n",
    "\n",
    "#Setup Hadoop\n",
    "export HADOOP_HOME=$HOME/hadoop-2.7.3\n",
    "export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
    "export HADOOP_MAPRED_HOME=$HADOOP_HOME\n",
    "export HADOOP_COMMON_HOME=$HADOOP_HOME\n",
    "export HADOOP_HDFS_HOME=$HADOOP_HOME\n",
    "export YARN_HOME=$HADOOP_HOME\n",
    "export PATH=$PATH:$HADOOP_HOME/bin\n",
    "\n",
    "#Launch Hadoop\n",
    "$HADOOP_HOME/sbin/start-dfs.sh\n",
    "$HADOOP_HOME/sbin/start-yarn.sh\n",
    "$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop Demo\n",
    "\n",
    "Now that the Hadoop services are running, you can insert data into the HDFS\n",
    "\n",
    "`$ hadoop fs -mkdir /input`\n",
    "\n",
    "`$ hadoop fs -put starcraft.csv /input/.`\n",
    "\n",
    "starcraft.csv is a database with information about Starcraft players, including their rank and age. Let's perform a Map Reduce operation on this dataset, to obtain the average age at each rank.\n",
    "\n",
    "To achieve this we have created a mapper.py script, which extracts the age and rank of the dataset, and a reducer.py script, which performs an averaging operation.\n",
    "\n",
    "They key item to note about this scripts is that they operate with the standard input/output facilites. This allows Hadoop to run several instances in parallel with different portions of the dataset\n",
    "\n",
    "Now to execute our code (the current directory has to contain the mapper.py and reducer.py scripts):\n",
    "\n",
    "`$ hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar -file mapper.py -file reducer.py -mapper mapper.py -reducer reducer.py -input /input/starcraft.csv -output /output`\n",
    "\n",
    "This will tell the Hadoop server to perform the operation on the dataset in a distributed cluster. Once all the Mapping and Reducing operations are done, a success message is printed:\n",
    "\n",
    "```\n",
    "19/07/16 15:42:19 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
    "packageJobJar: [mapper.py, reducer.py, /tmp/hadoop-unjar3585551381596376153/] [] /tmp/streamjob3165364863209447883.jar tmpDir=null\n",
    "19/07/16 15:42:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
    "19/07/16 15:42:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
    "19/07/16 15:42:20 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
    "19/07/16 15:42:20 INFO mapreduce.JobSubmitter: number of splits:2\n",
    "19/07/16 15:42:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1563301111758_0003\n",
    "19/07/16 15:42:21 INFO impl.YarnClientImpl: Submitted application application_1563301111758_0003\n",
    "19/07/16 15:42:21 INFO mapreduce.Job: The url to track the job: http://ubuntu:8088/proxy/application_1563301111758_0003/\n",
    "19/07/16 15:42:21 INFO mapreduce.Job: Running job: job_1563301111758_0003\n",
    "19/07/16 15:42:27 INFO mapreduce.Job: Job job_1563301111758_0003 running in uber mode : false\n",
    "19/07/16 15:42:27 INFO mapreduce.Job:  map 0% reduce 0%\n",
    "19/07/16 15:42:32 INFO mapreduce.Job:  map 100% reduce 0%\n",
    "19/07/16 15:42:37 INFO mapreduce.Job:  map 100% reduce 100%\n",
    "19/07/16 15:42:37 INFO mapreduce.Job: Job job_1563301111758_0003 completed successfully\n",
    "19/07/16 15:42:37 INFO mapreduce.Job: Counters: 49\n",
    "        File System Counters            \n",
    "                FILE: Number of bytes read=26726          \n",
    "                FILE: Number of bytes written=419739      \n",
    "                FILE: Number of read operations=0           \n",
    "                FILE: Number of large read operations=0\n",
    "                FILE: Number of write operations=0\n",
    "                HDFS: Number of bytes read=548941\n",
    "                HDFS: Number of bytes written=91\n",
    "                HDFS: Number of read operations=9\n",
    "                HDFS: Number of large read operations=0\n",
    "                HDFS: Number of write operations=2\n",
    "        Job Counters              \n",
    "                Launched map tasks=2\n",
    "                Launched reduce tasks=1\n",
    "                Data-local map tasks=2\n",
    "                Total time spent by all maps in occupied slots (ms)=6275\n",
    "                Total time spent by all reduces in occupied slots (ms)=2226\n",
    "                Total time spent by all map tasks (ms)=6275\n",
    "                Total time spent by all reduce tasks (ms)=2226           \n",
    "                Total vcore-milliseconds taken by all map tasks=6275       \n",
    "                Total vcore-milliseconds taken by all reduce tasks=2226\n",
    "                Total megabyte-milliseconds taken by all map tasks=6425600 \n",
    "                Total megabyte-milliseconds taken by all reduce tasks=2279424\n",
    "        Map-Reduce Framework\n",
    "                Map input records=3395\n",
    "                Map output records=3340\n",
    "                Map output bytes=20040\n",
    "                Map output materialized bytes=26732\n",
    "                Input split bytes=186\n",
    "                Combine input records=0\n",
    "                Combine output records=0\n",
    "                Reduce input groups=145\n",
    "                Reduce shuffle bytes=26732\n",
    "                Reduce input records=3340\n",
    "                Reduce output records=7\n",
    "                Spilled Records=6680\n",
    "                Shuffled Maps =2\n",
    "                Failed Shuffles=0\n",
    "                Merged Map outputs=2\n",
    "                GC time elapsed (ms)=189\n",
    "                CPU time spent (ms)=2120\n",
    "                Physical memory (bytes) snapshot=701128704\n",
    "                Virtual memory (bytes) snapshot=5860065280\n",
    "                Total committed heap usage (bytes)=533725184\n",
    "        Shuffle Errors\n",
    "                BAD_ID=0\n",
    "                CONNECTION=0\n",
    "                IO_ERROR=0\n",
    "                WRONG_LENGTH=0\n",
    "                WRONG_MAP=0\n",
    "                WRONG_REDUCE=0\n",
    "        File Input Format Counters\n",
    "                Bytes Read=548755\n",
    "        File Output Format Counters\n",
    "                Bytes Written=91\n",
    "19/07/16 15:42:37 INFO streaming.StreamJob: Output directory: /output\n",
    "```\n",
    "\n",
    "The results are stored in the directory indicated by the previous command:\n",
    "\n",
    "`$ hadoop fs -cat /output/part-00000`\n",
    "\n",
    "```\n",
    "1,22.724551                                  \n",
    "2,22.155620              \n",
    "3,22.050633               \n",
    "4,21.981504                                                            \n",
    "5,21.362283                     \n",
    "6,20.677939                                     \n",
    "7,21.171429   \n",
    "```\n",
    "\n",
    "Now you know enough to bring up a Hadoop environment, populate it with data and perform operations on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "### Spark Installation\n",
    "\n",
    "Follow the tutorial at https://www.edureka.co/blog/spark-tutorial/\n",
    "* Java is already installed from the previous step\n",
    "* spark-shell to run commands interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
