{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark + Hadoop + H2O Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the complexities of code involved, most of the examples below can't actually be run from the notebook. Where noted, copy the code shown into an appropiate script and execute from the console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop\n",
    "\n",
    "### Hadoop Installation\n",
    "\n",
    "Follow the tutorial at https://www.edureka.co/blog/install-hadoop-single-node-hadoop-cluster . Some notes to consider:\n",
    "* Download and install JDK 8u101 (from https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html , the version from the tutorial may not work for your processor)\n",
    "* Download and install Hadoop\n",
    "* Configure environment for use\n",
    "   * This configuration consists of editing xml files so that Hadoop creates a single node environment. They're pretty easy to follow\n",
    "   * Before starting the daemons, run the command `sudo apt-get install openssh-client openssh-server`. This is required to open port 22\n",
    "   * At the end of this tutorial, you will have a Hadoop instance running. Browse to http://localhost:50070/dfshealth.html to see it in action.\n",
    "   * SH script to configure environment and launch Hadoop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#Setup Java\n",
    "export JAVA_HOME=$HOME/jdk1.8.0_101\n",
    "export PATH=$JAVA_HOME/bin:$PATH\n",
    "\n",
    "#Setup Hadoop\n",
    "export HADOOP_HOME=$HOME/hadoop-2.7.3\n",
    "export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
    "export HADOOP_MAPRED_HOME=$HADOOP_HOME\n",
    "export HADOOP_COMMON_HOME=$HADOOP_HOME\n",
    "export HADOOP_HDFS_HOME=$HADOOP_HOME\n",
    "export YARN_HOME=$HADOOP_HOME\n",
    "export PATH=$PATH:$HADOOP_HOME/bin\n",
    "\n",
    "#Launch Hadoop\n",
    "$HADOOP_HOME/sbin/start-dfs.sh\n",
    "$HADOOP_HOME/sbin/start-yarn.sh\n",
    "$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop Demo\n",
    "\n",
    "Now that the Hadoop services are running, you can insert data into the HDFS\n",
    "\n",
    "`$ hadoop fs -mkdir /input`\n",
    "\n",
    "`$ hadoop fs -put starcraft.csv /input/.`\n",
    "\n",
    "starcraft.csv is a database with information about Starcraft players, including their rank and age. Let's perform a Map Reduce operation on this dataset, to obtain the average age at each rank.\n",
    "\n",
    "To achieve this we have created a mapper.py script, which extracts the age and rank of the dataset, and a reducer.py script, which performs an averaging operation.\n",
    "\n",
    "They key item to note about this scripts is that they operate with the standard input/output facilites. This allows Hadoop to run several instances in parallel with different portions of the dataset\n",
    "\n",
    "Now to execute our code (the current directory has to contain the mapper.py and reducer.py scripts):\n",
    "\n",
    "`$ hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar -file mapper.py -file reducer.py -mapper mapper.py -reducer reducer.py -input /input/starcraft.csv -output /output`\n",
    "\n",
    "This will tell the Hadoop server to perform the operation on the dataset in a distributed cluster. Once all the Mapping and Reducing operations are done, a success message is printed:\n",
    "\n",
    "```\n",
    "19/07/16 15:42:19 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
    "packageJobJar: [mapper.py, reducer.py, /tmp/hadoop-unjar3585551381596376153/] [] /tmp/streamjob3165364863209447883.jar tmpDir=null\n",
    "19/07/16 15:42:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
    "19/07/16 15:42:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
    "19/07/16 15:42:20 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
    "19/07/16 15:42:20 INFO mapreduce.JobSubmitter: number of splits:2\n",
    "19/07/16 15:42:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1563301111758_0003\n",
    "19/07/16 15:42:21 INFO impl.YarnClientImpl: Submitted application application_1563301111758_0003\n",
    "19/07/16 15:42:21 INFO mapreduce.Job: The url to track the job: http://ubuntu:8088/proxy/application_1563301111758_0003/\n",
    "19/07/16 15:42:21 INFO mapreduce.Job: Running job: job_1563301111758_0003\n",
    "19/07/16 15:42:27 INFO mapreduce.Job: Job job_1563301111758_0003 running in uber mode : false\n",
    "19/07/16 15:42:27 INFO mapreduce.Job:  map 0% reduce 0%\n",
    "19/07/16 15:42:32 INFO mapreduce.Job:  map 100% reduce 0%\n",
    "19/07/16 15:42:37 INFO mapreduce.Job:  map 100% reduce 100%\n",
    "19/07/16 15:42:37 INFO mapreduce.Job: Job job_1563301111758_0003 completed successfully\n",
    "19/07/16 15:42:37 INFO mapreduce.Job: Counters: 49\n",
    "        File System Counters            \n",
    "                FILE: Number of bytes read=26726          \n",
    "                FILE: Number of bytes written=419739      \n",
    "                FILE: Number of read operations=0           \n",
    "                FILE: Number of large read operations=0\n",
    "                FILE: Number of write operations=0\n",
    "                HDFS: Number of bytes read=548941\n",
    "                HDFS: Number of bytes written=91\n",
    "                HDFS: Number of read operations=9\n",
    "                HDFS: Number of large read operations=0\n",
    "                HDFS: Number of write operations=2\n",
    "        Job Counters              \n",
    "                Launched map tasks=2\n",
    "                Launched reduce tasks=1\n",
    "                Data-local map tasks=2\n",
    "                Total time spent by all maps in occupied slots (ms)=6275\n",
    "                Total time spent by all reduces in occupied slots (ms)=2226\n",
    "                Total time spent by all map tasks (ms)=6275\n",
    "                Total time spent by all reduce tasks (ms)=2226           \n",
    "                Total vcore-milliseconds taken by all map tasks=6275       \n",
    "                Total vcore-milliseconds taken by all reduce tasks=2226\n",
    "                Total megabyte-milliseconds taken by all map tasks=6425600 \n",
    "                Total megabyte-milliseconds taken by all reduce tasks=2279424\n",
    "        Map-Reduce Framework\n",
    "                Map input records=3395\n",
    "                Map output records=3340\n",
    "                Map output bytes=20040\n",
    "                Map output materialized bytes=26732\n",
    "                Input split bytes=186\n",
    "                Combine input records=0\n",
    "                Combine output records=0\n",
    "                Reduce input groups=145\n",
    "                Reduce shuffle bytes=26732\n",
    "                Reduce input records=3340\n",
    "                Reduce output records=7\n",
    "                Spilled Records=6680\n",
    "                Shuffled Maps =2\n",
    "                Failed Shuffles=0\n",
    "                Merged Map outputs=2\n",
    "                GC time elapsed (ms)=189\n",
    "                CPU time spent (ms)=2120\n",
    "                Physical memory (bytes) snapshot=701128704\n",
    "                Virtual memory (bytes) snapshot=5860065280\n",
    "                Total committed heap usage (bytes)=533725184\n",
    "        Shuffle Errors\n",
    "                BAD_ID=0\n",
    "                CONNECTION=0\n",
    "                IO_ERROR=0\n",
    "                WRONG_LENGTH=0\n",
    "                WRONG_MAP=0\n",
    "                WRONG_REDUCE=0\n",
    "        File Input Format Counters\n",
    "                Bytes Read=548755\n",
    "        File Output Format Counters\n",
    "                Bytes Written=91\n",
    "19/07/16 15:42:37 INFO streaming.StreamJob: Output directory: /output\n",
    "```\n",
    "\n",
    "The results are stored in the directory indicated by the previous command:\n",
    "\n",
    "`$ hadoop fs -cat /output/part-00000`\n",
    "\n",
    "```\n",
    "1,22.724551                                  \n",
    "2,22.155620              \n",
    "3,22.050633               \n",
    "4,21.981504                                                            \n",
    "5,21.362283                     \n",
    "6,20.677939                                     \n",
    "7,21.171429   \n",
    "```\n",
    "\n",
    "Now you know enough to bring up a Hadoop environment, populate it with data and perform operations on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "### Spark Installation\n",
    "\n",
    "Follow the tutorial at https://www.tutorialspoint.com/apache_spark/apache_spark_installation.htm\n",
    "* Java is already installed from the previous step\n",
    "* Instalation is simple: Extract the scala binaries and update PATH\n",
    "* At this point tools are available, such as spark-shell to run commands interactively\n",
    "* Shell script to set up environment:\n",
    "```\n",
    "#Spark\n",
    "export SPARK_HOME=$HOME/spark-2.4.3-bin-hadoop2.7\n",
    "export PATH=$PATH:$SPARK_HOME/bin\n",
    "```\n",
    "\n",
    "### Spark Demo\n",
    "\n",
    "Unfortunately, due to the configuration of this virtual machine, the Spark demo cannot be executed in place. We will show the code without actually running it.\n",
    "A good resource in addition to this demo is the following tutorial: https://towardsdatascience.com/apache-spark-mllib-tutorial-ec6f1cb336a9\n",
    "\n",
    "Now that Spark is installed, launch the interactive python Spark environment:\n",
    "\n",
    "`$ pyspark`\n",
    "\n",
    "This command will launch the spark-shell server. You should see a message along the lines of `Spark context Web UI available at http://192.168.110.128:4040` . You can browse to this address to manage the session.\n",
    "\n",
    "It is easier to work with Spark from a Python script using a couple of helper libraries:\n",
    "\n",
    "```\n",
    "$ conda install -c conda-forge pyspark findspark\n",
    "```\n",
    "\n",
    "Now we're ready to start working with some models. For this demo, Spark will work standalone, without Hadoop. First lets create a Spark session in which to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('$SPARK_HOME')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('spark_hadoop/starcraft.csv')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And extract some features. Note the VectorAssembler call, which loads the data into the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "feature_cols = data.columns[2:-1]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "features = assembler.transform(data)\n",
    "features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our features and labels, we can create a train/test split and run a linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "train, test = features.randomSplit([0.8, 0.2])\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"LeagueIndex\")\n",
    "model = lr.fit(train)\n",
    "result = model.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there it is! We have created an ML model using Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2O\n",
    "\n",
    "### Installation\n",
    "\n",
    "For installation you should follow the tutorial at http://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html (there are instructions also for Hadoop and Anaconda Cloud)\n",
    "\n",
    "#### Installation in Python\n",
    "\n",
    "Install dependencies (prepending with sudo if needed):\n",
    "* pip install requests\n",
    "* pip install tabulate\n",
    "* pip install \"colorama>=0.3.8\"\n",
    "* pip install future\n",
    "\n",
    "Install module\n",
    "\n",
    "* pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o\n",
    "\n",
    "Optionally initialize H2O in Python and run a demo to see H2O at work.\n",
    "\n",
    "* python\n",
    "* import h2o\n",
    "* h2o.init()\n",
    "* h2o.demo(\"glm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2O Demo\n",
    "\n",
    "Follow the tutorial at http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import h2o\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "from __future__ import print_function\n",
    "h2o.init()\n",
    "\n",
    "# Import a sample binary outcome train/test set into H2O\n",
    "train = h2o.import_file(\"https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv\")\n",
    "test = h2o.import_file(\"https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv\")\n",
    "\n",
    "# Identify predictors and response\n",
    "x = train.columns\n",
    "y = \"response\"\n",
    "x.remove(y)\n",
    "\n",
    "# For binary classification, response should be a factor\n",
    "train[y] = train[y].asfactor()\n",
    "test[y] = test[y].asfactor()\n",
    "\n",
    "# Number of CV folds (to generate level-one data for stacking)\n",
    "nfolds = 5\n",
    "\n",
    "# There are a few ways to assemble a list of models to stack together:\n",
    "# 1. Train individual models and put them in a list\n",
    "# 2. Train a grid of models\n",
    "# 3. Train several grids of models\n",
    "# Note: All base models must have the same cross-validation folds and\n",
    "# the cross-validated predicted values must be kept.\n",
    "\n",
    "\n",
    "# 1. Generate a 2-model ensemble (GBM + RF)\n",
    "\n",
    "# Train and cross-validate a GBM\n",
    "my_gbm = H2OGradientBoostingEstimator(distribution=\"bernoulli\",\n",
    "                                      ntrees=10,\n",
    "                                      max_depth=3,\n",
    "                                      min_rows=2,\n",
    "                                      learn_rate=0.2,\n",
    "                                      nfolds=nfolds,\n",
    "                                      fold_assignment=\"Modulo\",\n",
    "                                      keep_cross_validation_predictions=True,\n",
    "                                      seed=1)\n",
    "my_gbm.train(x=x, y=y, training_frame=train)\n",
    "\n",
    "\n",
    "# Train and cross-validate a RF\n",
    "my_rf = H2ORandomForestEstimator(ntrees=50,\n",
    "                                 nfolds=nfolds,\n",
    "                                 fold_assignment=\"Modulo\",\n",
    "                                 keep_cross_validation_predictions=True,\n",
    "                                 seed=1)\n",
    "my_rf.train(x=x, y=y, training_frame=train)\n",
    "\n",
    "\n",
    "# Train a stacked ensemble using the GBM and GLM above\n",
    "ensemble = H2OStackedEnsembleEstimator(model_id=\"my_ensemble_binomial\",\n",
    "                                       base_models=[my_gbm, my_rf])\n",
    "ensemble.train(x=x, y=y, training_frame=train)\n",
    "\n",
    "# Eval ensemble performance on the test data\n",
    "perf_stack_test = ensemble.model_performance(test)\n",
    "\n",
    "# Compare to base learner performance on the test set\n",
    "perf_gbm_test = my_gbm.model_performance(test)\n",
    "perf_rf_test = my_rf.model_performance(test)\n",
    "baselearner_best_auc_test = max(perf_gbm_test.auc(), perf_rf_test.auc())\n",
    "stack_auc_test = perf_stack_test.auc()\n",
    "print(\"Best Base-learner Test AUC:  {0}\".format(baselearner_best_auc_test))\n",
    "print(\"Ensemble Test AUC:  {0}\".format(stack_auc_test))\n",
    "\n",
    "# Generate predictions on a test set (if neccessary)\n",
    "pred = ensemble.predict(test)\n",
    "\n",
    "\n",
    "# 2. Generate a random grid of models and stack them together\n",
    "\n",
    "# Specify GBM hyperparameters for the grid\n",
    "hyper_params = {\"learn_rate\": [0.01, 0.03],\n",
    "                \"max_depth\": [3, 4, 5, 6, 9],\n",
    "                \"sample_rate\": [0.7, 0.8, 0.9, 1.0],\n",
    "                \"col_sample_rate\": [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]}\n",
    "search_criteria = {\"strategy\": \"RandomDiscrete\", \"max_models\": 3, \"seed\": 1}\n",
    "\n",
    "# Train the grid\n",
    "grid = H2OGridSearch(model=H2OGradientBoostingEstimator(ntrees=10,\n",
    "                                                        seed=1,\n",
    "                                                        nfolds=nfolds,\n",
    "                                                        fold_assignment=\"Modulo\",\n",
    "                                                        keep_cross_validation_predictions=True),\n",
    "                     hyper_params=hyper_params,\n",
    "                     search_criteria=search_criteria,\n",
    "                     grid_id=\"gbm_grid_binomial\")\n",
    "grid.train(x=x, y=y, training_frame=train)\n",
    "\n",
    "# Train a stacked ensemble using the GBM grid\n",
    "ensemble = H2OStackedEnsembleEstimator(model_id=\"my_ensemble_gbm_grid_binomial\",\n",
    "                                       base_models=grid.model_ids)\n",
    "ensemble.train(x=x, y=y, training_frame=train)\n",
    "\n",
    "# Eval ensemble performance on the test data\n",
    "perf_stack_test = ensemble.model_performance(test)\n",
    "\n",
    "# Compare to base learner performance on the test set\n",
    "baselearner_best_auc_test = max([h2o.get_model(model).model_performance(test_data=test).auc() for model in grid.model_ids])\n",
    "stack_auc_test = perf_stack_test.auc()\n",
    "print(\"Best Base-learner Test AUC:  {0}\".format(baselearner_best_auc_test))\n",
    "print(\"Ensemble Test AUC:  {0}\".format(stack_auc_test))\n",
    "\n",
    "# Generate predictions on a test set (if neccessary)\n",
    "pred = ensemble.predict(test)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
