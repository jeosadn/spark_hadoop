{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in High-Performance Computing Environments\n",
    "\n",
    "https://markdowntomedium.com/\n",
    "\n",
    "## Introduction\n",
    "\n",
    "* Talk about origins of high performance computing, what are clusters.\n",
    "* What is MapReduce\n",
    "* Installation of all dependencies and running of different daemons\n",
    "\n",
    "## Hadoop\n",
    "\n",
    "* What is hadoop, dates, google origins, language APIs\n",
    "* Hadoop is disk-based, batch operations\n",
    "* Components\n",
    "    * What is HDFS\n",
    "       * Mention data wrangling operations (mkdir, ls, etc)\n",
    "    * What is JobTracker - MapReduce operations\n",
    "        * How to submit and track jobs\n",
    "        * Event handling\n",
    "        * Task results\n",
    "        * Example of Map job followed by a Reduce Job\n",
    "    * What is Yarn\n",
    "        * Resource management - Client architecture\n",
    "        * Node and application manager\n",
    "        * Scheduler\n",
    "        * Cluster assignments\n",
    "        * Interaction with files\n",
    "* No ML so far, just large data wrangling\n",
    "* Demo should create a Hadoop cluster, load data into it, do some batch mapreduce jobs to demonstrate\n",
    "\n",
    "## Spark\n",
    "\n",
    "* What is Spark, language APIs\n",
    "* Deployment - Standalone, Yarn, MapReduce\n",
    "* Components\n",
    "    * Spark SQL\n",
    "    * Spark Streaming\n",
    "    * MLib\n",
    "    * GraphX\n",
    "* Architecture\n",
    "    * Resilient Distributed Data Sets\n",
    "    * Advatages in regard to data sharing\n",
    "    * Iterative vs Interactive operations\n",
    "    * Spark operations\n",
    "        * Actions\n",
    "        * Transformations\n",
    "        * Broadcast variables\n",
    "        * Accumulators\n",
    "        * Numeric Operations\n",
    "* Using the Spark Shell and the REST API\n",
    "* Spark is ram-based, flexible operations in memory. More performance heavy\n",
    "* What does Spark does different from Hadoop\n",
    "* How does Spark work together with Hadoop\n",
    "* What does Spark does better than Hadoop and vice versa\n",
    "* Sparks machine learning library\n",
    "* Demo should take Hadoop's data, load it into RAM, operate on it\n",
    "\n",
    "## H2O\n",
    "\n",
    "* What is H2O, native Java, Scala, REST API for Python, others.\n",
    "   * A scalable machine learning platform that interfaces with Hadoop and Scala\n",
    "* Pulling data to and from Hadoop/Spark\n",
    "* Components\n",
    "    * Using H2O from Python\n",
    "    * Using H2O Flow (basically Jupyter)\n",
    "    * Using H2O Sparkling Water\n",
    "* ML results\n",
    "* Demo should do some ML on Spark's data\n",
    "\n",
    "http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
